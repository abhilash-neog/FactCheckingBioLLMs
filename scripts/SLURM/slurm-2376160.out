Resetting modules to system default. Reseting $MODULEPATH back to system default. All extra directories will be removed from $MODULEPATH.
Resetting modules to system default. Reseting $MODULEPATH back to system default. All extra directories will be removed from $MODULEPATH.
/cm/local/apps/slurm/var/spool/job2376160/slurm_script: line 15: activate: No such file or directory
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Model:  asclepius
Running for the dataset:  FCT
Batch size: 12
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]/home/amartya/.conda/envs/med/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  33%|███▎      | 1/3 [00:14<00:29, 14.76s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:27<00:13, 13.58s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:38<00:00, 12.50s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:38<00:00, 12.91s/it]
Generating answers for the data
  0%|          | 0/1573 [00:00<?, ?it/s]/home/amartya/.conda/envs/med/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/amartya/.conda/envs/med/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
  0%|          | 1/1573 [00:18<7:54:26, 18.11s/it]  0%|          | 2/1573 [00:29<6:13:17, 14.26s/it]  0%|          | 3/1573 [01:48<19:05:24, 43.77s/it]  0%|          | 4/1573 [01:58<13:14:31, 30.38s/it]  0%|          | 5/1573 [02:09<10:07:55, 23.26s/it]  0%|          | 6/1573 [03:38<19:58:54, 45.91s/it]  0%|          | 7/1573 [03:48<14:48:38, 34.05s/it]  1%|          | 8/1573 [04:00<11:48:39, 27.17s/it]  1%|          | 9/1573 [04:07<8:59:22, 20.69s/it]   1%|          | 10/1573 [04:21<8:09:05, 18.78s/it]  1%|          | 11/1573 [05:56<18:11:21, 41.92s/it]  1%|          | 12/1573 [06:22<16:02:24, 36.99s/it]  1%|          | 13/1573 [06:37<13:10:09, 30.39s/it]This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (2048). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.
  1%|          | 14/1573 [08:13<21:45:32, 50.25s/it]  1%|          | 15/1573 [08:24<16:39:08, 38.48s/it]  1%|          | 16/1573 [09:55<23:24:21, 54.12s/it]  1%|          | 17/1573 [11:25<28:09:53, 65.16s/it]  1%|          | 18/1573 [11:39<21:25:57, 49.62s/it]  1%|          | 19/1573 [11:48<16:08:33, 37.40s/it]  1%|▏         | 20/1573 [12:01<13:01:01, 30.17s/it]  1%|▏         | 21/1573 [13:33<21:01:58, 48.79s/it]  1%|▏         | 22/1573 [13:40<15:37:00, 36.25s/it]  1%|▏         | 23/1573 [13:55<12:51:53, 29.88s/it]  2%|▏         | 24/1573 [14:05<10:17:26, 23.92s/it]  2%|▏         | 25/1573 [14:19<9:01:21, 20.98s/it]   2%|▏         | 26/1573 [15:49<17:54:41, 41.68s/it]  2%|▏         | 27/1573 [15:56<13:23:03, 31.17s/it]  2%|▏         | 28/1573 [16:20<12:30:09, 29.13s/it]  2%|▏         | 29/1573 [17:49<20:09:50, 47.01s/it]  2%|▏         | 30/1573 [18:03<15:50:46, 36.97s/it]  2%|▏         | 31/1573 [18:17<12:51:56, 30.04s/it]  2%|▏         | 32/1573 [19:47<20:37:48, 48.19s/it]  2%|▏         | 33/1573 [19:58<15:47:01, 36.90s/it]  2%|▏         | 34/1573 [21:27<22:27:43, 52.54s/it]  2%|▏         | 35/1573 [21:43<17:48:14, 41.67s/it]  2%|▏         | 36/1573 [23:13<24:00:49, 56.25s/it]  2%|▏         | 37/1573 [24:45<28:31:00, 66.84s/it]  2%|▏         | 38/1573 [24:55<21:11:25, 49.70s/it]  2%|▏         | 39/1573 [25:08<16:34:21, 38.89s/it]  3%|▎         | 40/1573 [25:19<13:00:54, 30.56s/it]  3%|▎         | 41/1573 [25:35<11:07:49, 26.16s/it]  3%|▎         | 42/1573 [25:48<9:22:18, 22.04s/it]   3%|▎         | 43/1573 [26:00<8:10:17, 19.23s/it]  3%|▎         | 44/1573 [26:14<7:28:53, 17.62s/it]  3%|▎         | 45/1573 [27:43<16:29:18, 38.85s/it]  3%|▎         | 46/1573 [29:14<23:13:11, 54.74s/it]  3%|▎         | 47/1573 [29:24<17:25:36, 41.11s/it]  3%|▎         | 48/1573 [30:51<23:19:09, 55.05s/it]  3%|▎         | 49/1573 [31:04<17:54:39, 42.31s/it]  3%|▎         | 50/1573 [31:16<14:00:47, 33.12s/it]  3%|▎         | 51/1573 [31:27<11:19:15, 26.78s/it]  3%|▎         | 52/1573 [31:41<9:34:42, 22.67s/it]   3%|▎         | 53/1573 [31:47<7:31:39, 17.83s/it]  3%|▎         | 54/1573 [32:05<7:29:07, 17.74s/it]  3%|▎         | 55/1573 [32:14<6:26:08, 15.26s/it]  4%|▎         | 56/1573 [32:25<5:49:55, 13.84s/it]  4%|▎         | 57/1573 [33:54<15:25:46, 36.64s/it]  4%|▎         | 58/1573 [34:08<12:33:29, 29.84s/it]  4%|▍         | 59/1573 [34:15<9:38:19, 22.92s/it]   4%|▍         | 60/1573 [34:30<8:32:52, 20.34s/it]  4%|▍         | 61/1573 [35:58<17:08:56, 40.83s/it]  4%|▍         | 62/1573 [36:06<12:57:43, 30.88s/it]  4%|▍         | 63/1573 [36:21<11:00:32, 26.25s/it]  4%|▍         | 64/1573 [37:49<18:42:45, 44.64s/it]  4%|▍         | 65/1573 [39:18<24:18:21, 58.02s/it]  4%|▍         | 66/1573 [39:25<17:50:57, 42.64s/it]  4%|▍         | 67/1573 [41:01<24:29:51, 58.56s/it]  4%|▍         | 68/1573 [41:13<18:40:39, 44.68s/it]  4%|▍         | 69/1573 [41:23<14:18:07, 34.23s/it]  4%|▍         | 70/1573 [41:40<12:12:01, 29.22s/it]  5%|▍         | 71/1573 [41:59<10:50:10, 25.97s/it]  5%|▍         | 72/1573 [42:15<9:36:52, 23.06s/it]   5%|▍         | 73/1573 [42:24<7:49:50, 18.79s/it]  5%|▍         | 74/1573 [42:27<5:49:41, 14.00s/it]  5%|▍         | 75/1573 [43:56<15:14:01, 36.61s/it]  5%|▍         | 76/1573 [45:25<21:44:28, 52.28s/it]slurmstepd: error: *** JOB 2376160 ON tc-gpu001 CANCELLED AT 2024-04-27T17:58:03 ***
